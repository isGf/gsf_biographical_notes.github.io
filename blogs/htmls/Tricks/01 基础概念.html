<!DOCTYPE html>
<html lang="zh">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>基础概念</title>
    <style>
        /* 复用 tutorial.html 的所有样式 */
        body {
            margin: 0;
            padding: 0;
            font-family: Arial, sans-serif;
            line-height: 1.6;
            color: #333;
        }

        .navbar {
            background-color: #fff;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
            padding: 10px 0;
            position: fixed;
            width: 100%;
            top: 0;
            z-index: 1000;
            display: flex;
            align-items: center;
            justify-content: space-between;
            height: 60px;
        }

        .logo {
            margin-left: 40px;
        }

        .logo img {
            width: 120px;
            height: auto;
            display: block;
        }

        .page-title {
            font-size: 24px;
            color: #2c3e50;
            margin: 0;
            font-weight: bold;
        }

        .back-button {
            margin-right: 40px;
            padding: 8px 16px;
            background-color: #3498db;
            color: white;
            text-decoration: none;
            border-radius: 5px;
            transition: background-color 0.3s;
            font-weight: bold;
        }

        .back-button:hover {
            background-color: #2980b9;
        }

        /* 页面容器和导航样式 */
        .page-container {
            display: flex;
            margin-top: 80px;
            padding-left: 250px;
        }

        .side-nav {
            width: 250px;
            height: calc(100vh - 80px);
            position: fixed;
            left: 0;
            top: 80px;
            background-color: #f8f9fa;
            padding: 20px;
            box-shadow: 2px 0 5px rgba(0,0,0,0.1);
            overflow-y: auto;
        }

        .side-nav ul {
            list-style: none;
            padding: 0;
            margin: 0;
        }

        .side-nav li {
            margin-bottom: 15px;
        }

        .side-nav a {
            color: #333;
            text-decoration: none;
            display: block;
            padding: 10px;
            border-radius: 5px;
            transition: all 0.3s ease;
        }

        .side-nav a:hover {
            background-color: #e9ecef;
            color: #007bff;
        }

        .side-nav a.active {
            background-color: #007bff;
            color: white;
        }

        .tutorial-content {
            flex: 1;
            max-width: 800px;
            margin: 0 auto;
            padding: 20px;
            background-color: #fff;
            box-shadow: 0 0 10px rgba(0,0,0,0.1);
            border-radius: 8px;
        }

        .tutorial-section h2 {
            color: #3498db;
            font-size: 1.8em;
            margin: 30px 0 20px;
            padding-bottom: 10px;
            border-bottom: 2px solid #3498db;
        }

        .tutorial-section p {
            font-size: 1.1em;
            line-height: 1.8;
            margin-bottom: 15px;
            color: #555;
        }

        .tutorial-section ul {
            padding-left: 20px;
        }

        .tutorial-section li {
            margin-bottom: 10px;
            font-size: 1.1em;
            color: #555;
        }

        /* 点赞部分样式 */
        .like-section {
            text-align: center;
            margin: 60px 0;
            padding: 20px;
            position: relative;
            overflow: hidden;
        }

        .like-button {
            font-size: 56px;
            background: none;
            border: none;
            cursor: pointer;
            transition: all 0.3s ease;
            padding: 10px;
            position: relative;
            display: inline-block;
        }

        .like-button:hover {
            transform: translateY(-5px);
        }

        .like-button.liked {
            animation: likeEffect 0.5s ease;
        }

        @keyframes likeEffect {
            0% { transform: scale(1); }
            25% { transform: scale(1.2) rotate(-15deg); }
            50% { transform: scale(0.95) rotate(15deg); }
            75% { transform: scale(1.1) rotate(-15deg); }
            100% { transform: scale(1) rotate(0); }
        }

        .like-text {
            margin-top: 20px;
            font-size: 18px;
            color: #666;
            font-family: "PingFang SC", "Microsoft YaHei", sans-serif;
        }

        .gradient-dash {
            background: linear-gradient(45deg, #ff6b6b, #4ecdc4);
            -webkit-background-clip: text;
            background-clip: text;
            color: transparent;
            font-weight: bold;
            padding: 0 8px;
            font-size: 20px;
        }

        footer {
            background-color: #2c3e50;
            color: white;
            text-align: center;
            padding: 20px 0;
            position: relative;
            bottom: 0;
            width: 100%;
        }

        .footer-bottom {
            padding: 10px 0;
        }
    </style>
</head>
<body>
    <header>
        <nav class="navbar">
            <div class="logo">
                <img src="HawkLogo.png" alt="HAWK AI Logo" />
            </div>
            <h1 class="page-title">最佳实践指南</h1>
            <a href="../../../index.html" class="back-button">返回首页</a>
        </nav>
    </header>

    <div class="page-container">
        <!-- 左侧导航 -->
        <nav class="side-nav">
            <ul>
                <li><a href="#jcrz" class="active">AI基础认知</a></li>
                <li><a href="#l-e-r">LLM-Embedding-Ranker</a></li>
                <li><a href="#prompt">Prompt 提示词工程</a></li>
                <li><a href="#r-a-g">RAG 检索增强生成</a></li>
                <li><a href="#fine-tuning">Fine-Tuning 微调</a></li>
                <li><a href="#fine-tuning-fun">常见微调算法</a></li>
                <li><a href="#model-compress">大模型压缩技术</a></li>
                <li><a href="#web-ai">人工智能行业资讯</a></li>
                <li><a href="#other">其他资料</a></li>
            </ul>
        </nav>

        <main class="tutorial-content">
            <div class="tutorial-section">
                <div id="jcrz">
                    <h2 style="text-align: center;">AI基础认知</h2>
                        <h3>1. 通用大模型和垂类大模型</h3>
                        <ul>
                            <p><b>通用大模型</b></p>
                            <ul>
                                <li>通用大模型，即General AI Model，是指那些经过广泛训练，能够处理多种任务和领域的人工智能模型。这些模型通常基于大规模的数据集，旨在具备一定的普遍性和适应性。</li>
                                <li>优点：能够处理多种任务（如文本生成、翻译、问答等），适合多种应用场景，被广泛应用于自然语言处理、图像识别、语音识别等领域。可以通过微调适应特定需求，支持各种行业的应用。</li>
                                <li>缺点：通用大模型虽然经过大量数据的训练，但这些数据通常无法深入特定领域，并且训练数据来自互联网和其他广泛的文本源，其中可能包含各种偏见、刻板印象和错误信息。这些偏见和错误可能会反映在模型的输出中。</li>
                            </ul>
                        </ul>
                        <ul>
                            <p><b>垂类大模型</b></p>
                            <ul>
                                <li>垂类大模型是针对特定领域或行业进行优化和训练的人工智能模型。这些模型通常在特定的数据集上进行训练，以便更好地满足行业需求。</li>
                                <li>优点：针对特定数据集进行训练，使其在该领域内表现卓越。由于训练数据的专业性，模型生成的内容通常更少出现错误，能够避免通用模型可能出现的模糊或误导性回答。</li>
                                <li>缺点：由于垂类模型的表现高度依赖于训练数据的质量和数量，若数据不足或存在错误，模型可能无法提供可靠的结果。开发专门的垂类模型需要较高的投入，包括数据收集、标注和持续维护，成本相对较高</li>
                            </ul>
                        </ul>
                        <h3>2. 大模型的参数及类别</h3>
                        <ul>
                            <p><b>1. 规模分类</b></p>
                            <ul>
                                <li>3B和7B分别表示模型的参数量为30亿和70亿。参数数量越多，模型的复杂度和学习能力越强，但也对计算资源有更高要求，且训练时间也更长。</li>
                            </ul>
                        </ul>
                        <ul>
                            <p><b>2. 领域分类</b></p>
                            <ul>
                                <li>自然语言处理模型：专注于文本相关任务，如ChatGPT、ChatGLM等；</li>
                                <li>语音模型：文字相互转换语音任务，如Chattts等；</li>
                                <li>多模态模型：可以处理多种非文字数据；如QwenVL等；</li>
                                <li>媒体生成模型：文生图、文生视频；如SD、CogVideo等；</li>
                            </ul>
                        </ul>
                        <ul>
                            <p><b>3. 开源与闭源</b></p>
                            <ul>
                                <li>开源模型：允许进行修改和私有化使用，如Qwen、GLM、Hunyuan；</li>
                                <li>闭源模型：只能在线使用，但一般拥有超级性能，如Kimi、文心一言；</li>
                            </ul>
                        </ul>
                </div>

                <div id="l-e-r">
                    <h2 style="text-align: center;">LLM-Embedding-Ranker</h2>
                    <ul>
                        <p><b>1. 大语言模型（LLM）</b></p>
                        <ul>
                            <li>LLM是指通过大规模数据训练的语言模型，能够生成、理解和处理自然语言文本。</li>
                            <li>LLM模型用于语义理解和内容生成</li>
                        </ul>
                    </ul>
                    <ul>
                        <p><b>2. 嵌入模型（Embedding）</b></p>
                        <ul>
                            <li>Embedding模型主要用于将词、短语或句子转换为向量形式，以便计算机能够处理和理解。</li>
                            <li>Embedding模型提供语义表示</li>
                        </ul>
                    </ul>
                    <ul>
                        <p><b>3. 排序模型（Ranker）</b></p>
                        <ul>
                            <li>Ranker模型用于对候选项进行排序，通常在信息检索或推荐系统中使用。</li>
                            <li>Ranker模型确保最相关的内容被优先呈现</li>
                        </ul>
                    </ul>
                </div>

                <div id="prompt">
                    <h2 style="text-align: center;">Prompt 提示词工程</h2>
                    <ul>
                        <p><b>1. Prompt对模型的影响</b></p>
                    <ul>
                        <li>Prompt能够引导大模型生成更符合预期的内容，例如身份设定、任务内容等</li>
                    </ul>
                        <p><b>2. 不同场景下的Prompt设计</b></p>
                    <ul>
                        <li>在公司智能助理的场景中，Prompt的设计应注重回答的准确性；在AI答辩场景中，Prompt更应注重用户问答的上下文信息设计。</li>
                    </ul>
                        <p><b>3. 如何设计有效的Prompt</b></p>
                    <ul>
                        <li>设置开场白：让大模型先介绍自己</li>
                        <li>清晰表达：使用“总结以下内容”而不是“说点什么”</li>
                        <li>示范样例：提供示例输入和期望输出，让模型理解格式和风格</li>
                    </ul>
                        <p><b>4. 提示词工程学习资源推荐</b></p>
                        <ul>
                            <li><a href="https://waytoagi.feishu.cn/wiki/Q5mXww4rriujFFkFQOzc8uIsnah?table=tbldSgFt2xNUDNAz&view=vewo2g2ktO" target="_blank">Prompts(提示词) - 飞书云文档</a></li>
                            <li><a href="https://zhuanlan.zhihu.com/p/665165976" target="_blank">从1000+模板中总结出的10大提示工程方法助你成为提示词大师！ - 知乎</a></li>
                        </ul>
                    </ul>
                </div>

                <div id="r-a-g">
                    <h2 style="text-align: center;">RAG 检索增强生成</h2>
                    <ul>
                        <p><b>1. RAG 技术定义</b></p>
                        <p>　　RAG（Retrieve-Augment-Generate）技术是一种基于检索、增强和生成的文本生成方法。通过结合检索模型和生成模型，提高文本生成的准确性和连贯性。</p>
                        <ul>
                            <ul>
                                <p>Retrieval 检索：</p>
                                    <ul><li>信息检索。主要任务是从外部知识库中检索相关信息。帮助模型找到与用户查询相关的事实或数据，为生成提供背景信息。</li></ul>
                                <p>Augmented 增强：</p>
                                    <ul><li>将检索的信息和模型能力以及Prompt结合起来，提供额外的上下文，增强生成模型的回答能力，使得生成的内容更加准确和相关。</li></ul>
                                <p>Generation 生成：</p>
                                    <ul><li>模型利用先前的步骤所获取的知识来生成流畅且符合上下文的回答。</li></ul>
                                </ul>
                        </ul>
                        <p><b>2. 知识库</b></p>
                        <ul>
                            <li>知识库（Knowledge）是一些文档的集合。一个知识库可以被整体集成至一个应用中作为检索上下文使用。</li>
                            <li>大模型在语言生成和上下文理解方面，由于过度依赖于固定的预训练数据，它们在回答需要特定领域的问题时，可能会出现“编造”信息的现象，导致生成结果不准确或缺乏事实依据。</li>
                        </ul>
                        <p><b>3. 知识图谱</b></p>
                        <ul>
                            <p>基本概念：</p>
                                <ul><li>知识图谱（Knowledge Graph，KG）是一种揭示实体之间关系的语义网络，可以对现实世界的事物及其相互关系进行形式化地描述。</li></ul>
                            <p>知识图谱是什么：</p>
                                <ul>
                                    <li>知识图谱是一个存储信息的方法</li>
                                    <li>描述的是事实</li>
                                    <li>核心表达的是实体和关系</li>
                                    <li>可以有多种实体和关系</li>
                                </ul>
                            <p>知识图谱的缺点：</p>
                                <ul>
                                    <li>知识图谱构建工作量大</li>
                                    <li>大模型生成知识图谱本身就会有幻觉问题</li>
                                    <li>知识更新难度极大</li>
                                </ul>
                        </ul>
                        <p><b>4. 知识库和知识图谱结合</b></p>
                        <ul>
                            <p>知识库的问题：</p>
                                <ul><li>知识库中的数据检索通常依赖于关键词匹配，可能导致检索结果不够精确或不够丰富。</li></ul>
                            <p>知识图谱的问题：</p>
                                <ul><li>知识图谱在某些专业领域的构建可能会受到数据的限制，无法覆盖所有细节。</li></ul>
                            <p>知识库+知识图谱的结果：</p>
                                <ul><li>为知识库引入知识图谱的图形化结构，将知识库中的数据相互关联，使得信息不仅仅是简单的事实，呈现出实体之间的多维关系和上下文语境。</li></ul>
                        </ul>
                    </ul>
                    <div style="text-align: left;">
                        <img src="RAG流程图.png" style="max-width: 100%; height: auto;">
                    </div>
               </div>

                <div id="fine-tuning">
                    <h2 style="text-align: center;">Fine-Tuning 微调</h2>
                    <ul>
                        <p><b>微调核心概念</b></p>
                        <p>　　大模型微调（Fine-tuning）是指在已经预训练好的大型深度学习模型基础上，使用新的、特定任务相关的数据集对模型进行进一步训练的过程。主要目的是使模型能够适应新的、具体的任务或领域，而无需从头开始训练一个全新的模型。</p>
                    <ul>
                            <p>优点：</p>
                                <ul><li>模型可以专注于具体任务的细节，从而提高在特定任务上的表现，在特定任务中更好地捕捉任务相关的特征和模式，也可以使模型快速适应新的领域或任务。</li></ul>
                            <p>缺点：</p>
                                <ul><li>微调大型预训练模型需要较为强大的硬件支持，如7b的模型选择全量微调和不做量化版至少需要120G的显存，另外在微调过程中，模型有可能忘记以前学到的通用知识，尤其是在微调数据与以前的数据差异较大的情况下。</li></ul>
                        </ul>
                    </ul>
                    <div style="text-align: left;">
                        <img src="通用微调流程图.png" style="max-width: 100%; height: auto;">
                    </div>
                </div>

                <div id="fine-tuning-fun">
                    <h2 style="text-align: center;">常见微调算法</h2>
                    <ul>
                        <p><b>Full Fine-Tuning全量微调</b></p>
                        <p>　　全量微调是指对预训练模型的所有参数进行调整和优化。在这种方法中，模型的所有权重（包括网络中的每一层和每个连接的权重）都会在新任务的训练过程中被更新。</p>
                            <ul>
                                <ul>
                                    <li>优点：</li>
                                    <ul>
                                        <li>在数据充足的情况下，能够充分利用微调数据，提高模型的表现。</li>
                                    </ul>
                                    <li>缺点：</li>
                                    <ul>
                                        <li>需要大量的计算资源和存储空间。</li>
                                    </ul>
                                </ul>
                            </ul>
                        <p><b>Low-Rank Adaptation 低秩适配微调</b></p>
                        <p>　　LoRA微调是一种参数高效的微调方法。在LoRA中模型的每一层的权重矩阵会被分解成低秩矩阵的和，并且仅更新这些低秩矩阵的参数，而保持原始的预训练权重不变。</p>
                            <ul>
                                <ul>
                                    <li>优点：</li>
                                    <ul>
                                        <li>相比全量微调，所需的计算资源和存储空间大大减少。</li>
                                        <li>不会对预训练模型的结构产生太大影响。</li>
                                    </ul>
                                    <li>缺点：</li>
                                    <ul>
                                        <li>任务难度较大时无法达到预期。</li>
                                        <li>产生更多的幻觉问题。</li>
                                    </ul>
                                </ul>
                            </ul>
                        <p><b>Supervised Fine-Tuning 监督微调</b></p>
                        <p>　　是指已有的预训练模型（大语言模型）已经经过大量无监督预训练之后，使用带标签的训练数据进行进一步的训练，目的是利用特定领域或特定任务的数据，进一步提升模型在该任务上的表现。</p>
                            <ul>
                                <ul>
                                    <li>优点：</li>
                                    <ul>
                                        <li>指导模型在更特定任务上生成更准确和更相关的回答。</li>
                                    </ul>
                                    <li>缺点：</li>
                                    <ul>
                                        <li>监督微调依赖于高质量、充分标注的数据集。</li>
                                    </ul>
                                </ul>
                            </ul>
                        <div style="text-align: left;">
                            <img src="实施微调流程图.png" style="max-width: 100%; height: auto;">
                        </div>
                    </ul>
                </div>

                <div id="model-compress">
                    <h2 style="text-align: center;">大模型压缩技术</h2>
                    <ul>
                        <p><b>量化（Quantization）：</b></p>
                            <ul>
                                <li>减少模型中数值表示精度的方式来减小模型大小和推理过程的技术</li>
                                <ul>
                                    <li>降低硬件要求</li>
                                    <li>量化可能会导致一定的精度损失</li>
                                </ul>
                            </ul>
                        <p><b>蒸馏（Knowledge Distillation）：</b></p>
                            <ul>
                                <li>通过让一个较小的模型模仿一个较大且性能更好的模型的行为，从而使得小模型学习到大模型的“知识”</li>
                                <ul>
                                    <li>让小模型拥有大模型的能力</li>
                                    <li>小模型的训练过程依赖于大模型的能力</li>
                                </ul>
                            </ul>
                        <p><b>剪枝（Pruning）：</b></p>
                            <ul>
                                <li>去除神经网络中不重要或冗余的权重连接来减少模型大小</li>
                                <ul>
                                    <li>剪枝后模型更小，计算更高效</li>
                                    <li>剪枝过程需对每一层、每一个参数的剪除进行仔细设计</li>
                                </ul>
                            </ul>
                    </ul>
                </div>

                <div id="web-ai">
                    <h2 style="text-align: center;">人工智能行业资讯</h2>
                    <ul>
                        <p>
                            <b>人工智能行业重要资讯　（2024年6月 ~ 至今）</b>
                            <ul>
                                <li><b>2022年11月30日</b></li>
                                <ul><li>ChatGPT问世！OpenAI全新对话式AI模型ChatGPT正式发布</li></ul>
                                <li><b>2024年6月</b></li>
                                <ul><li>皓客航空正式踏入人工智能赛道</li></ul>
                                <ul><li>中国生成式人工智能用户规模增长：截至2024年6月，中国生成式人工智能产品的用户规模已经达到了2.3亿人，占全国总人口的16.4%</li></ul>
                                <li><b>2024年7月</b></li>
                                <ul><li>2024世界人工智能大会召开：7月4日至6日，2024世界人工智能大会在上海召开，聚焦大模型、算力、机器人、自动驾驶等“AI+”热门领域</li></ul>
                                <ul><li>中国生成式人工智能服务大模型数量：截至2024年7月，中国已上线并备案的生成式人工智能服务大模型超过190个</li></ul>
                                <li><b>2024年8月</b></li>
                                <ul><li>智谱AI宣布开源了CogVideoX视频生成模型。将AI视频领域的竞争推向了新的高度。</li></ul>
                                <li><b>2024年9月</b></li>
                                <ul><li>Gartner发布技术成熟度曲线：Gartner发布2024年中国数据、分析和人工智能技术成熟度曲线，预测未来两到五年内，大量具有颠覆性或较高影响力的创新技术可能会实现主流采用</li></ul>
                                <ul><li>AI领域动态：包括谷歌和Meta发布新的AI模型，AI语音机器人的发展等</li></ul>
                                <li><b>2024年10月</b></li>
                                <ul><li>全球人工智能领域的重大事件盘点：包括Meta的Llama 3.2模型展示、《2024年人工智能指数报告》发布等</li></ul>
                                <ul><li>《2024 人工智能十大前沿技术趋势展望》发布：10月23日，在北京发布，涵盖了AI共性技术、大规模预训练模型、具身智能以及生成式人工智能等多个前沿领域</li></ul>
                                <ul><li>华为推出AI处理器“昇腾 5”：10月29日，华为宣布推出其最新的AI处理器“昇腾 5”，在性能和能效上均实现了突破</li></ul>
                                <li><b>2024年11月</b></li>
                                <ul><li>中国信通院发布《人工智能发展报告（2024年）》：报告深入分析了人工智能技术的最新发展趋势和未来展望</li></ul>
                                <ul><li>中国生成式AI大会（上海站）：12月5-6日，2024中国生成式AI大会（上海站）「GenAICon 2024」将在上海中星铂尔曼大酒店举办</li></ul>
                                <li><b>2024年12月</b></li>
                                <ul><li>全国工商联人工智能委员会成立：12月2日，全国工商联人工智能委员会在南京正式成立，旨在贯彻落实党的二十届三中全会精神，扩大统战工作在人工智能领域的影响</li></ul>
                                <ul><li>《2024年人工智能十大前沿技术趋势展望》发布：在2024年世界科技与发展论坛期间发布，旨在构建开放合作的人工智能技术发展趋势</li></ul>
                            </ul>
                        </p>
                    </ul>
                </div>

                <div id="other">
                    <h2 style="text-align: center;">其他资料</h2>
                    <ul>
                        <p>
                            <b>皓客内部AI技术分享</b>
                            <ul>
                                <li>第一次：基础认知及Prompt提示词工程</li>
                                <li>第二次：知识库、知识图谱和RAG</li>
                                <li>第三次：大模型压缩及微调技术</li>
                                <ul><li>PPT地址（公司服务器）：\\SERVER\01 HAWK_Public\07  培训类\03 公司内训课程\AI技术应用系列培训\01 AI 技术分享-高赛菲</li></ul>
                            </ul>
                        </p>
                    </ul>
                </div>

                <!-- 点赞部分 -->
                <div class="like-section">
                    <button class="like-button" id="likeButton">👍</button>
                    <div class="like-text">
                        <span class="gradient-dash">——</span>
                        真诚点赞，手留余香
                        <span class="gradient-dash">——</span>
                    </div>
                </div>
            </div>
        </main>
    </div>

    <footer>
        <div class="footer-bottom">
            <p>&copy; 2024 HAWK.AI 保留所有权利。</p>
        </div>
    </footer>

    <script>
        // 导航高亮和滚动效果
        document.addEventListener('DOMContentLoaded', () => {
            const navLinks = document.querySelectorAll('.side-nav a');
            
            // 监听滚动事件
            window.addEventListener('scroll', () => {
                const fromTop = window.scrollY + 100;

                navLinks.forEach(link => {
                    const section = document.querySelector(link.hash);
                    
                    if (section.offsetTop <= fromTop &&
                        section.offsetTop + section.offsetHeight > fromTop) {
                        link.classList.add('active');
                    } else {
                        link.classList.remove('active');
                    }
                });
            });

            navLinks.forEach(link => {
                link.addEventListener('click', (e) => {
                    e.preventDefault();
                    const targetId = link.getAttribute('href');
                    const targetSection = document.querySelector(targetId);
                    const navbarHeight = document.querySelector('.navbar').offsetHeight;
                    
                    window.scrollTo({
                        top: targetSection.offsetTop - navbarHeight,
                        behavior: 'smooth'
                    });
                });
            });
        });

        // 点赞功能
        const likeButton = document.getElementById('likeButton');
        let isLiked = false;

        likeButton.addEventListener('click', () => {
            isLiked = !isLiked;
            if (isLiked) {
                likeButton.classList.add('liked');
                setTimeout(() => {
                    likeButton.classList.remove('liked');
                }, 500);
            }
        });
    </script>
</body>
</html> 